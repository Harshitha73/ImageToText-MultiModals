# ImageToText-MultiModals

The code takes a video as the input, reads the 20th frame of that video and displays the output using two multimodals 
1. vikhyatk/moondream2
2. llava-hf/llava-1.5-7b-hf

I've used google colab to run this code, changed the runtime type to "T4 GPU".
The given "output.pdf" is the resultant output for "SampleVideo.mp4". The pdf displays the 20th frame of the video and the descriptions generated by 2 models.
