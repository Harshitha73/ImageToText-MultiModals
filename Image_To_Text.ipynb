{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPP0k7QpvQonaHKoh8jTkQ0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harshitha73/ImageToText-MultiModals/blob/main/Image_To_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lYPxDv6UbEAm"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet transformers\n",
        "!pip install --quiet einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet bitsandbytes\n",
        "!pip install --quiet git+https://github.com/huggingface/transformers.git # Install latest version of transformers\n",
        "!pip install --quiet accelerate"
      ],
      "metadata": {
        "id": "mdMCfR3JbbHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "from IPython.display import HTML, display\n",
        "import base64\n",
        "import io\n",
        "\n",
        "\n",
        "# Load the Moondream model\n",
        "moondream_model_id = \"vikhyatk/moondream2\"\n",
        "revision = \"2024-04-02\"\n",
        "moondream_model = AutoModelForCausalLM.from_pretrained(\n",
        "    moondream_model_id, trust_remote_code=True, revision=revision\n",
        ")\n",
        "moondream_tokenizer = AutoTokenizer.from_pretrained(moondream_model_id, revision=revision)\n",
        "\n",
        "# Pipeline for \"image-to-text\" using the Llava model\n",
        "# Use \"bitsandbytes\" library to quantize the model\n",
        "pipe = pipeline(\"image-to-text\", model_kwargs= {\"device_map\": \"auto\", \"load_in_8bit\": True}, model=\"llava-hf/llava-1.5-7b-hf\")\n",
        "\n",
        "print(\"Please upload the video file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if len(uploaded) == 0:\n",
        "    print(\"No files uploaded. Please try again.\")\n",
        "else:\n",
        "    # Get the uploaded video file\n",
        "    video_path = list(uploaded.keys())[0]  # Selects the first item (assuming only one file is uploaded)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    # Get the 20th frame\n",
        "    frame_number = 20\n",
        "    current_frame = 0\n",
        "    while cap.isOpened():\n",
        "      ret, frame = cap.read()\n",
        "      if not ret:\n",
        "        break\n",
        "      current_frame += 1\n",
        "      if current_frame == frame_number:\n",
        "        cv2.imwrite('frame_20.png', frame)\n",
        "        break\n",
        "    cap.release()\n",
        "\n",
        "    # Load the saved image\n",
        "    image_path = 'frame_20.png'\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    # Apply the Moondream model to describe the image\n",
        "    moondream_enc_image = moondream_model.encode_image(image)\n",
        "    moondream_description = moondream_model.answer_question(moondream_enc_image, \"Describe the image\", moondream_tokenizer)\n",
        "\n",
        "\n",
        "    llava_description = pipe(image, prompt=\"<image> Describe the image\", generate_kwargs={\"max_new_tokens\": 200})\n",
        "    generated_text = llava_description[0]['generated_text']\n",
        "    prefix = \"Describe the image. (100 words) The\"\n",
        "    prefix_index = generated_text.find(prefix) + len(prefix)\n",
        "\n",
        "    # Remove the prefix and suffix\n",
        "    clean_llava_description = generated_text[prefix_index:].strip('}[')\n",
        "\n",
        "    # Convert the image to a data URL\n",
        "    buffer = io.BytesIO()\n",
        "    image.save(buffer, format=\"PNG\")\n",
        "    img_str = \"data:image/png;base64,\" + base64.b64encode(buffer.getvalue()).decode()\n",
        "\n",
        "    # Construct HTML strings for displaying the 20th frame and descriptions generated by both models\n",
        "    image_display = f'<img src=\"{img_str}\" style=\"width:50%; height:\"10%\"; margin-right:10px;\">'\n",
        "    moondream_display_description = f'<div><h3>Moondream description:</h3><p>{moondream_description}</p></div>'\n",
        "    llava_display_description = f'<div><h3>Llava description:</h3><p>{clean_llava_description}</p></div>'\n",
        "    popup_content = f'<div style=\"width:100%; overflow:auto;\">{image_display}{moondream_display_description}{llava_display_description}</div>'\n",
        "    # Click on the \"Popup\" button to display the output\n",
        "    display(HTML('<script>function openPopup(){var myWindow = window.open(\"\", \"Popup\", \"width=800,height=400\"); myWindow.document.body.innerHTML = `%s`;}</script><button onclick=\"openPopup()\">Open Popup</button>' % popup_content))"
      ],
      "metadata": {
        "id": "PY8xrtYlbdJJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
